{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "%pip install -U openai==0.28.1 \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#instantiate a chat\n",
    "llm = ChatOpenAI()\n",
    "#llm.predict('How are you?')\n",
    "\n",
    "#check version \n",
    "openai.__version__\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "api_key=os.getenv('OPENAI_API_KEY') #set as an environmental variable\n",
    "#openai.api_key=('personal key')\n",
    "#Load credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain==0.0.331 --ignore-installed PyYAML\n",
    "import langchain \n",
    "langchain.__version__\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationChain \n",
    "\n",
    "#prompt a new chained conversation with the LLM\n",
    "chat_model=ChatOpenAI()\n",
    "\n",
    "chain=ConversationChain(\n",
    "    llm=chat_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"How are you today?\")\n",
    "\n",
    "chain.run(\"What was my current conversation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "Return all the subcategories of the following category\n",
    "\n",
    "{category}\n",
    "\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    input_variables=[\"category\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a helpful assistant who generates comma separated lists.\n",
    "The user will only pass a category and you should generate subcategories\n",
    "ONLY return comma separated and nothing more!\n",
    "\"\"\"\n",
    "\n",
    "human_template='{category}'\n",
    "\n",
    "system_message=SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_message=HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message, human_message\n",
    "])\n",
    "\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedParser(BaseOutputParser):\n",
    "    def parse(self,text):\n",
    "        #remove whitespaces, and split strings with commas inbetween\n",
    "        output=text.strip().split(',')\n",
    "        output=[o.strip() for o in output]\n",
    "        return output\n",
    "\n",
    "#the output should be a more clean list\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    output_parser=CommaSeparatedParser(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "input_list=[\n",
    "    {'category':'food'},\n",
    "    {'category':'country'},\n",
    "    {'category':'colors'}\n",
    "]\n",
    "\n",
    "#chained response of categories option\n",
    "response=chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_template=\"\"\"\n",
    "You are a writer. Given a subject, your job is to return a fun title for a play\n",
    "\n",
    "Subject: {subject}\n",
    "Title:\"\"\"\n",
    "\n",
    "title_chain=LLMChain.from_string(\n",
    "    llm=chat_model,\n",
    "    template=title_template\n",
    ")\n",
    "\n",
    "title_chain.run(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_template=\"\"\"\n",
    "You are a writer.\n",
    "Given a title, write a synopsis for a play.\n",
    "\n",
    "Title: {title}\n",
    "Synopsis:\n",
    "\"\"\"\n",
    "\n",
    "synopsis_chain=LLMChain.from_string(\n",
    "    llm=chat_model,\n",
    "    template=synopsis_template\n",
    ")\n",
    "\n",
    "#input the outputs of the original title into the synposis chain\n",
    "title=\"Generated Title\"\n",
    "\n",
    "synopsis_chain.run(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential chain version\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain=SimpleSequentialChain(\n",
    "    chains=[title_chain, synopsis_chain],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"Machine Learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.refine_llm_chain.prompt.template)\n",
    "\n",
    "initial_template=\"\"\"\n",
    "Extract the most relevant themes from the following:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "THEMES:\"\"\"\n",
    "\n",
    "refine_template=\"\"\"\n",
    "Your job is to extract the most relevant themes\n",
    "We have provided an existing list of themes up to a certain point: {existing_answer}\n",
    "We have the opportunity to refine the existing list (only if needed) with some context below\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "Given the new context, refine the original list\n",
    "If the context isn't useful, return the original list and ONLY the original list.\n",
    "Return that list as a comma separated list.\n",
    "\n",
    "LIST:\"\"\"\n",
    "\n",
    "initial_prompt=PromptTemplate.from_template(initial_template)\n",
    "refine_prompt=PromptTemplate.from_template(refine_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing data sources\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm=ChatOpenAI()\n",
    "chain=load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\", #use \"map_reduce\" for more complex documents\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "%pip install langchain openai tqdm jq unstructured pypdf tiktoken \n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    UnstructuredCSVLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    PythonLoader,\n",
    "    PyPDFLoader,\n",
    "    JSONLoader\n",
    ")\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import DirectoryLoader \n",
    "\n",
    "#file_path='csv file path'\n",
    "csv_loader=CSVLoader(file_path=file_path)\n",
    "compiled_data=csv_loader.load()\n",
    "compiled_data[0].page_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents\n",
    "file_path=\"/Users/anthonychen/Desktop/Big data and social science a practical guide to methods and tools by Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter, Julia Lane.pdf\"\n",
    "sl_loader=PyPDFLoader(file_path)\n",
    "#split into chunks for LLM processing\n",
    "data_chunks=loader.load_and_split() #uses recursive character text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map reduce strategy for large amounts of data for LLM to process\n",
    "#or refine chunks with separate summaries & combine at the end\n",
    "\n",
    "from langchain.text_splitter import(\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "\n",
    "splitter1=CharacterTextSplitter(\n",
    "    chunk_size=1000, #1k characters\n",
    "    chunk_overlap=0, \n",
    ")\n",
    "\n",
    "#more chunks than previous:\n",
    "splitter2=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "s1_data1=sl_loader.load_and_split(text_splitter=splitter1)\n",
    "s1_data2=sl_loader.load_and_split(text_splitter=splitter2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary works better on csv file (when unrefined)\n",
    "#chain.run(compiled_data[:5])\n",
    "chain.run(sl_data1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path=\"\"\n",
    "mixed_loader=DirectoryLoader(\n",
    "    path=folder_path,\n",
    "    use_multithreading=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "mixed_data=mixed_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 6. M achine Learning\n",
      "the expected value of the predictions of a classiﬁer and select t he\n",
      "model that optimizes this cost-sensitive metric.\n",
      "6.7 Practical tips\n",
      "Here we highlight some practical tips that will be helpful when w ork-\n",
      "ing with machine learning methods.\n",
      "6.7.1 Features\n",
      "So far in this chapter, we have focused a lot on methods and pro-\n",
      "ces\n",
      "s, and we have not discussed features in detail. In social science,\n",
      "they are not called features but instead are known as variables or\n",
      "predictors. \n"
     ]
    }
   ],
   "source": [
    "page=pages[200]\n",
    "print(page.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/anthonychen/Desktop/Big data and social science a practical guide to methods and tools by Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter, Julia Lane.pdf',\n",
       " 'page': 200}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Ffl8b_GfJ-M&ab_channel=TobiasFischer\n",
      "[youtube] Ffl8b_GfJ-M: Downloading webpage\n",
      "[youtube] Ffl8b_GfJ-M: Downloading ios player API JSON\n",
      "[youtube] Ffl8b_GfJ-M: Downloading android player API JSON\n",
      "[youtube] Ffl8b_GfJ-M: Downloading m3u8 information\n",
      "[info] Ffl8b_GfJ-M: Downloading 1 format(s): 140\n",
      "[download] /Users/anthonychen/Documents/Youtube//Stable Diffusion Consistent Character Animation Technique - Tutorial.m4a has already been downloaded\n",
      "[download] 100% of   31.65MiB\n",
      "[ExtractAudio] Not converting audio /Users/anthonychen/Documents/Youtube//Stable Diffusion Consistent Character Animation Technique - Tutorial.m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-57325c91b3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/anthonychen/Documents/Youtube/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenericLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYoutubeAudioLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAIWhisperParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/generic.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;34m\"\"\"Load all documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def load_and_split(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/generic.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myield_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/parsers/audio.py\u001b[0m in \u001b[0;36mlazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Transcribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Transcribing part {split_number+1}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whisper-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             yield Document(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/audio.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(cls, model, file, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transcriptions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         return util.convert_to_openai_object(\n\u001b[1;32m     67\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "#brew install ffprobe and ffmpeg; youtube set rate limits\n",
    "url=\"https://www.youtube.com/watch?v=Ffl8b_GfJ-M&ab_channel=TobiasFischer\"\n",
    "save_dir=\"/Users/anthonychen/Documents/Youtube/\"\n",
    "loader=GenericLoader(YoutubeAudioLoader([url],save_dir), OpenAIWhisperParser())\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#website chatting\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://adriangcoder.medium.com/pandas-tricks-and-tips-a7b87c3748ea\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas for time series data — tricks and tips | by Adrian G | MediumPandas for time series data — tricks and tipsAdrian G·Follow7 min read·Oct 24, 2018--2ListenShareThere are some Pandas DataFrame manipulations that I keep looking up how to do. I am recording these here to save myself time. These may help you too.Time series dataConvert column to datetime with given formatdf[‘day_time’] = pd.to_datetime(df[‘day_time’], format=’%Y-%m-%d %H:%M:%S’)0 2012–10–12 00:00:001 2012–10–12 00:30:002 2012–1\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500]) #need to do postprocessing on info to get workable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notion databases\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader=NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-59fb17fc507e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-Shot Learning**\n",
    "\n",
    "As you feed the model 10 or more examples, the accuracy improves drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "#insert a few examples\n",
    "examples=[\n",
    "    {\"input\":\"\" , \"output\":\"\"},\n",
    "    {\"input\":\"\" , \"output\":\"\"},\n",
    "    {\"input\":\"\" , \"output\":\"\"}\n",
    "]\n",
    "\n",
    "example_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",\"{input}\"),\n",
    "        (\"ai\",\"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt=FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are wonderous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Input Question\n",
    "print(final_prompt.format(input=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=final_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memetic Proxy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "System: (reference)\n",
    "Provide a helpful answer to the following question:\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate.from_template(template)\n",
    "\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "high_level=\"Imagine you are a Professor teaching at the PhD level\"\n",
    "lower_level=\"Imagine you are a kindergarten teacher\"\n",
    "\n",
    "question=\"Explain Quantum Mechanics\"\n",
    "\n",
    "chain.run(\n",
    "    {\n",
    "        'question':question,\n",
    "        'reference':high_level\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings(show_progress_bar=True)\n",
    "\n",
    "vector1=embeddings.embed_query('How are you?')\n",
    "\n",
    "#embeddings.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm \n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    return np.dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "vector1=embeddings.embed_query('machine learning')\n",
    "vector2=embeddings.embed_query('artificial intelligence')\n",
    "cosine_sim=get_cosine(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS \n",
    "\n",
    "#choose document data to load\n",
    "index=FAISS.from_documents(data, embeddings)\n",
    "\n",
    "index.similarity_search_with_relevance_scores(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database retrieval\n",
    "\n",
    "Question converted to embedding, then search within index (created with Pinecone) of the vector database to get nearest neighbors, pass them into a prompt, provide prompt to LLM and get answer to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "retriever=index.as_retriever()\n",
    "#number of vectors to retrieve\n",
    "retriever.search_kwargs['fetch_k']=20\n",
    "#diversify the information provided to LLM\n",
    "retriever.search_kwargs['maximal_marginal_relevance']=True\n",
    "#final number of data context vectors provided\n",
    "retriever.search_kwargs['k']=10\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is machine learning?\",\n",
    "          callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV\n",
    ")\n",
    "\n",
    "#name originally input in pinecone\n",
    "index_name=''\n",
    "db=Pinecone.from_documents(\n",
    "    data, #context provided from document loader\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is machine learning?\",\n",
    "          callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sources for LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "newsapi=NewsApiClient(api_key=NEWS_API_KEY)\n",
    "today=date.today()\n",
    "last_week=today-timedelta(days=7)\n",
    "\n",
    "#dictionary of responses \n",
    "latest_news=newsapi.get_everything(\n",
    "    q='artifical intelligence',\n",
    "    from_param=last_week.strftime('%Y-%m-%d'),\n",
    "    to=today.strftime('%Y-%m-%d'),\n",
    "    sort_by='relevancy',\n",
    "    language='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document \n",
    "docs=[\n",
    "    Document( \n",
    "    page_content=article['title']+'\\n\\n'+article['description'],\n",
    "    metadata={\n",
    "        'source':article['url'],\n",
    "    }\n",
    "        , ) for article in latest_news['articles']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "qa_chain=create_qa_with_sources_chain(llm)\n",
    "\n",
    "doc_prompt=PromptTemplate(\n",
    "    template='Content: {page_content}\\nSource:{source}',\n",
    "    input_variables=['page_content','source'],\n",
    ")\n",
    "\n",
    "final_qa_chain=StuffDocumentsChain(\n",
    "    llm_chain=qa_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")\n",
    "\n",
    "index=FAISS.from_documents(docs, )\n",
    "\n",
    "chain=RetrievalQA(\n",
    "    retriever=index.as_retriever(),\n",
    "    combine_documents_chain=final_qa_chain\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"\"\"What is the most important news about artificial intelligence in the last week?\"\"\"\n",
    "\n",
    "answer=chain.run(question)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install apify-client chromadb #webcrawler & local vector db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.document_loeaders.base import Document \n",
    "\n",
    "apify=ApifyWrapper()\n",
    "\n",
    "loader=apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\n",
    "        \"startUrls\":[{\"url\":\"\"}], #insert url \n",
    "        \"aggressivePrune\":True,\n",
    "    },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item['text'] or \"\", metadata={\"source\":item['url']}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "index=VectorstoreIndexCreator(\n",
    "    text_splitter=text_splitter\n",
    ").from_loaders([loader])\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is the main subject of this ...\"\n",
    "\n",
    "index.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever=index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing GitHub Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader \n",
    "\n",
    "loader=GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./data/repo/\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "    branch='master',\n",
    ")\n",
    "\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "\n",
    "python_splitter=RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "documents=python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=FAISS.from_documents(documents, embeddings)\n",
    "retriever=index.as_retriever()\n",
    "\n",
    "#distance metric\n",
    "retriever.search_kwargs['distance_metric']='cos'\n",
    "#number of vectors to retrieve\n",
    "retriever.search_kwargs['fetch_k']=200\n",
    "#diversify the information provided to LLM\n",
    "retriever.search_kwargs['maximal_marginal_relevance']=True\n",
    "#final number of data context vectors provided\n",
    "retriever.search_kwargs['k']=10\n",
    "\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "index=Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "\n",
    "#map_rerank returns answer with highest score\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='stuff', #map-reduce, refine,map_rerank\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    '?', #insert Question\n",
    "    callbacks=[StdOutcallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
