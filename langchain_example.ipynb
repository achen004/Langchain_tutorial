{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "%pip install -U openai==0.28.1 \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#instantiate a chat\n",
    "llm = ChatOpenAI()\n",
    "#llm.predict('How are you?')\n",
    "\n",
    "#check version \n",
    "openai.__version__\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "api_key=os.getenv('OPENAI_API_KEY') #set as an environmental variable\n",
    "#openai.api_key=('personal key')\n",
    "#Load credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain==0.0.331 --ignore-installed PyYAML\n",
    "import langchain \n",
    "langchain.__version__\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationChain \n",
    "\n",
    "#prompt a new chained conversation with the LLM\n",
    "chat_model=ChatOpenAI()\n",
    "\n",
    "chain=ConversationChain(\n",
    "    llm=chat_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"How are you today?\")\n",
    "\n",
    "chain.run(\"What was my current conversation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "Return all the subcategories of the following category\n",
    "\n",
    "{category}\n",
    "\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    input_variables=[\"category\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a helpful assistant who generates comma separated lists.\n",
    "The user will only pass a category and you should generate subcategories\n",
    "ONLY return comma separated and nothing more!\n",
    "\"\"\"\n",
    "\n",
    "human_template='{category}'\n",
    "\n",
    "system_message=SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_message=HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    system_message, human_message\n",
    "])\n",
    "\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedParser(BaseOutputParser):\n",
    "    def parse(self,text):\n",
    "        #remove whitespaces, and split strings with commas inbetween\n",
    "        output=text.strip().split(',')\n",
    "        output=[o.strip() for o in output]\n",
    "        return output\n",
    "\n",
    "#the output should be a more clean list\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    output_parser=CommaSeparatedParser(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "input_list=[\n",
    "    {'category':'food'},\n",
    "    {'category':'country'},\n",
    "    {'category':'colors'}\n",
    "]\n",
    "\n",
    "#chained response of categories option\n",
    "response=chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_template=\"\"\"\n",
    "You are a writer. Given a subject, your job is to return a fun title for a play\n",
    "\n",
    "Subject: {subject}\n",
    "Title:\"\"\"\n",
    "\n",
    "title_chain=LLMChain.from_string(\n",
    "    llm=chat_model,\n",
    "    template=title_template\n",
    ")\n",
    "\n",
    "title_chain.run(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_template=\"\"\"\n",
    "You are a writer.\n",
    "Given a title, write a synopsis for a play.\n",
    "\n",
    "Title: {title}\n",
    "Synopsis:\n",
    "\"\"\"\n",
    "\n",
    "synopsis_chain=LLMChain.from_string(\n",
    "    llm=chat_model,\n",
    "    template=synopsis_template\n",
    ")\n",
    "\n",
    "#input the outputs of the original title into the synposis chain\n",
    "title=\"Generated Title\"\n",
    "\n",
    "synopsis_chain.run(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential chain version\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain=SimpleSequentialChain(\n",
    "    chains=[title_chain, synopsis_chain],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"Machine Learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.refine_llm_chain.prompt.template)\n",
    "\n",
    "initial_template=\"\"\"\n",
    "Extract the most relevant themes from the following:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "THEMES:\"\"\"\n",
    "\n",
    "refine_template=\"\"\"\n",
    "Your job is to extract the most relevant themes\n",
    "We have provided an existing list of themes up to a certain point: {existing_answer}\n",
    "We have the opportunity to refine the existing list (only if needed) with some context below\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "Given the new context, refine the original list\n",
    "If the context isn't useful, return the original list and ONLY the original list.\n",
    "Return that list as a comma separated list.\n",
    "\n",
    "LIST:\"\"\"\n",
    "\n",
    "initial_prompt=PromptTemplate.from_template(initial_template)\n",
    "refine_prompt=PromptTemplate.from_template(refine_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing data sources\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm=ChatOpenAI()\n",
    "chain=load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\", #use \"map_reduce\" for more complex documents\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "%pip install langchain openai tqdm jq unstructured pypdf tiktoken \n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    UnstructuredCSVLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    PythonLoader,\n",
    "    PyPDFLoader,\n",
    "    JSONLoader\n",
    ")\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import DirectoryLoader \n",
    "\n",
    "#file_path='csv file path'\n",
    "csv_loader=CSVLoader(file_path=file_path)\n",
    "compiled_data=csv_loader.load()\n",
    "compiled_data[0].page_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents\n",
    "file_path=\"/Users/anthonychen/Desktop/Big data and social science a practical guide to methods and tools by Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter, Julia Lane.pdf\"\n",
    "sl_loader=PyPDFLoader(file_path)\n",
    "#split into chunks for LLM processing\n",
    "data_chunks=loader.load_and_split() #uses recursive character text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map reduce strategy for large amounts of data for LLM to process\n",
    "#or refine chunks with separate summaries & combine at the end\n",
    "\n",
    "from langchain.text_splitter import(\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "\n",
    "splitter1=CharacterTextSplitter(\n",
    "    chunk_size=1000, #1k characters\n",
    "    chunk_overlap=0, \n",
    ")\n",
    "\n",
    "#more chunks than previous:\n",
    "splitter2=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "s1_data1=sl_loader.load_and_split(text_splitter=splitter1)\n",
    "s1_data2=sl_loader.load_and_split(text_splitter=splitter2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary works better on csv file (when unrefined)\n",
    "#chain.run(compiled_data[:5])\n",
    "chain.run(sl_data1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path=\"\"\n",
    "mixed_loader=DirectoryLoader(\n",
    "    path=folder_path,\n",
    "    use_multithreading=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "mixed_data=mixed_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 6. M achine Learning\n",
      "the expected value of the predictions of a classiﬁer and select t he\n",
      "model that optimizes this cost-sensitive metric.\n",
      "6.7 Practical tips\n",
      "Here we highlight some practical tips that will be helpful when w ork-\n",
      "ing with machine learning methods.\n",
      "6.7.1 Features\n",
      "So far in this chapter, we have focused a lot on methods and pro-\n",
      "ces\n",
      "s, and we have not discussed features in detail. In social science,\n",
      "they are not called features but instead are known as variables or\n",
      "predictors. \n"
     ]
    }
   ],
   "source": [
    "page=pages[200]\n",
    "print(page.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/anthonychen/Desktop/Big data and social science a practical guide to methods and tools by Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter, Julia Lane.pdf',\n",
       " 'page': 200}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Ffl8b_GfJ-M&ab_channel=TobiasFischer\n",
      "[youtube] Ffl8b_GfJ-M: Downloading webpage\n",
      "[youtube] Ffl8b_GfJ-M: Downloading ios player API JSON\n",
      "[youtube] Ffl8b_GfJ-M: Downloading android player API JSON\n",
      "[youtube] Ffl8b_GfJ-M: Downloading m3u8 information\n",
      "[info] Ffl8b_GfJ-M: Downloading 1 format(s): 140\n",
      "[download] /Users/anthonychen/Documents/Youtube//Stable Diffusion Consistent Character Animation Technique - Tutorial.m4a has already been downloaded\n",
      "[download] 100% of   31.65MiB\n",
      "[ExtractAudio] Not converting audio /Users/anthonychen/Documents/Youtube//Stable Diffusion Consistent Character Animation Technique - Tutorial.m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-57325c91b3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/anthonychen/Documents/Youtube/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenericLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYoutubeAudioLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAIWhisperParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/generic.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;34m\"\"\"Load all documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def load_and_split(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/generic.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myield_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/parsers/audio.py\u001b[0m in \u001b[0;36mlazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Transcribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Transcribing part {split_number+1}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whisper-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             yield Document(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_resources/audio.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(cls, model, file, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transcriptions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         return util.convert_to_openai_object(\n\u001b[1;32m     67\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "#brew install ffprobe and ffmpeg; youtube set rate limits\n",
    "url=\"https://www.youtube.com/watch?v=Ffl8b_GfJ-M&ab_channel=TobiasFischer\"\n",
    "save_dir=\"/Users/anthonychen/Documents/Youtube/\"\n",
    "loader=GenericLoader(YoutubeAudioLoader([url],save_dir), OpenAIWhisperParser())\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#website chatting\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://adriangcoder.medium.com/pandas-tricks-and-tips-a7b87c3748ea\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas for time series data — tricks and tips | by Adrian G | MediumPandas for time series data — tricks and tipsAdrian G·Follow7 min read·Oct 24, 2018--2ListenShareThere are some Pandas DataFrame manipulations that I keep looking up how to do. I am recording these here to save myself time. These may help you too.Time series dataConvert column to datetime with given formatdf[‘day_time’] = pd.to_datetime(df[‘day_time’], format=’%Y-%m-%d %H:%M:%S’)0 2012–10–12 00:00:001 2012–10–12 00:30:002 2012–1\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500]) #need to do postprocessing on info to get workable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notion databases\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader=NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-59fb17fc507e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-Shot Learning**\n",
    "\n",
    "As you feed the model 10 or more examples, the accuracy improves drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "#insert a few examples\n",
    "examples=[\n",
    "    {\"input\":\"\" , \"output\":\"\"},\n",
    "    {\"input\":\"\" , \"output\":\"\"},\n",
    "    {\"input\":\"\" , \"output\":\"\"}\n",
    "]\n",
    "\n",
    "example_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",\"{input}\"),\n",
    "        (\"ai\",\"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt=FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are wonderous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Input Question\n",
    "print(final_prompt.format(input=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=final_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memetic Proxy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "System: (reference)\n",
    "Provide a helpful answer to the following question:\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate.from_template(template)\n",
    "\n",
    "chain=LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "high_level=\"Imagine you are a Professor teaching at the PhD level\"\n",
    "lower_level=\"Imagine you are a kindergarten teacher\"\n",
    "\n",
    "question=\"Explain Quantum Mechanics\"\n",
    "\n",
    "chain.run(\n",
    "    {\n",
    "        'question':question,\n",
    "        'reference':high_level\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings(show_progress_bar=True)\n",
    "\n",
    "vector1=embeddings.embed_query('How are you?')\n",
    "\n",
    "#embeddings.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm \n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    return np.dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "vector1=embeddings.embed_query('machine learning')\n",
    "vector2=embeddings.embed_query('artificial intelligence')\n",
    "cosine_sim=get_cosine(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS \n",
    "\n",
    "#choose document data to load\n",
    "index=FAISS.from_documents(data, embeddings)\n",
    "\n",
    "index.similarity_search_with_relevance_scores(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database retrieval\n",
    "\n",
    "Question converted to embedding, then search within index (created with Pinecone) of the vector database to get nearest neighbors, pass them into a prompt, provide prompt to LLM and get answer to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "retriever=index.as_retriever()\n",
    "#number of vectors to retrieve\n",
    "retriever.search_kwargs['fetch_k']=20\n",
    "#diversify the information provided to LLM\n",
    "retriever.search_kwargs['maximal_marginal_relevance']=True\n",
    "#final number of data context vectors provided\n",
    "retriever.search_kwargs['k']=10\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is machine learning?\",\n",
    "          callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV\n",
    ")\n",
    "\n",
    "#name originally input in pinecone\n",
    "index_name=''\n",
    "db=Pinecone.from_documents(\n",
    "    data, #context provided from document loader\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is machine learning?\",\n",
    "          callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sources for LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "newsapi=NewsApiClient(api_key=NEWS_API_KEY)\n",
    "today=date.today()\n",
    "last_week=today-timedelta(days=7)\n",
    "\n",
    "#dictionary of responses \n",
    "latest_news=newsapi.get_everything(\n",
    "    q='artifical intelligence',\n",
    "    from_param=last_week.strftime('%Y-%m-%d'),\n",
    "    to=today.strftime('%Y-%m-%d'),\n",
    "    sort_by='relevancy',\n",
    "    language='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document \n",
    "docs=[\n",
    "    Document( \n",
    "    page_content=article['title']+'\\n\\n'+article['description'],\n",
    "    metadata={\n",
    "        'source':article['url'],\n",
    "    }\n",
    "        , ) for article in latest_news['articles']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "qa_chain=create_qa_with_sources_chain(llm)\n",
    "\n",
    "doc_prompt=PromptTemplate(\n",
    "    template='Content: {page_content}\\nSource:{source}',\n",
    "    input_variables=['page_content','source'],\n",
    ")\n",
    "\n",
    "final_qa_chain=StuffDocumentsChain(\n",
    "    llm_chain=qa_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")\n",
    "\n",
    "index=FAISS.from_documents(docs, )\n",
    "\n",
    "chain=RetrievalQA(\n",
    "    retriever=index.as_retriever(),\n",
    "    combine_documents_chain=final_qa_chain\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"\"\"What is the most important news about artificial intelligence in the last week?\"\"\"\n",
    "\n",
    "answer=chain.run(question)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install apify-client chromadb #webcrawler & local vector db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.document_loeaders.base import Document \n",
    "\n",
    "apify=ApifyWrapper()\n",
    "\n",
    "loader=apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\n",
    "        \"startUrls\":[{\"url\":\"\"}], #insert url \n",
    "        \"aggressivePrune\":True,\n",
    "    },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item['text'] or \"\", metadata={\"source\":item['url']}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "index=VectorstoreIndexCreator(\n",
    "    text_splitter=text_splitter\n",
    ").from_loaders([loader])\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is the main subject of this ...\"\n",
    "\n",
    "index.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever=index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing GitHub Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader \n",
    "\n",
    "loader=GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./data/repo/\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "    branch='master',\n",
    ")\n",
    "\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "\n",
    "python_splitter=RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "documents=python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=FAISS.from_documents(documents, embeddings)\n",
    "retriever=index.as_retriever()\n",
    "\n",
    "#distance metric\n",
    "retriever.search_kwargs['distance_metric']='cos'\n",
    "#number of vectors to retrieve\n",
    "retriever.search_kwargs['fetch_k']=200\n",
    "#diversify the information provided to LLM\n",
    "retriever.search_kwargs['maximal_marginal_relevance']=True\n",
    "#final number of data context vectors provided\n",
    "retriever.search_kwargs['k']=10\n",
    "\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "index=Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "\n",
    "#map_rerank returns answer with highest score\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='stuff', #map-reduce, refine,map_rerank\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    '?', #insert Question\n",
    "    callbacks=[StdOutcallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Optimization and Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U unstructured-inference onnx pyyesseract python-poppler chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multivector retrieval\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path=''\n",
    "loader=PyPDFLoader(file_path=file_path)\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "sl_data=loader.load_and_split(text_splitter=text_splitter)\n",
    "sl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "#store vectors\n",
    "vectorstore=Chroma(\n",
    "    collection_name=\"statistical_learning\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "#store data\n",
    "store=InMemoryStore()\n",
    "id_key='doc_id'\n",
    "\n",
    "retriever=MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "#create unique ids for each document in the data\n",
    "doc_ids=[str(uuid.uuid4()) for _ in sl_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10k character semantic information is very diluted\n",
    "#solution: break into smaller \n",
    "child_text_splitter=RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "all_sub_docs=[]\n",
    "for i, doc in enumerate(sl_data):\n",
    "    doc_id=doc_ids[i]\n",
    "    sub_docs=child_text_splitter.split_documents([doc])\n",
    "    for sub_doc in sub_docs:\n",
    "        #sub documents get parent id key\n",
    "        sub_doc.metadata[id_key]=doc_id\n",
    "    all_sub_docs.extend(sub_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass smaller documents\n",
    "retriever.vectorstore.add_documents(all_sub_docs)\n",
    "#pass parent documents\n",
    "retriever.docstore.mset(list(zip(doc_ids, sl_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter text or topic \n",
    "retriever.vectorstore.similarity_search(\"\")\n",
    "\n",
    "#get more relevant documents that are more informative\n",
    "retriever.get_relevant_documents(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(temperature=0)\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#insert question\n",
    "chain.run(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothetical questions for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import NumberedListOutputParser\n",
    "\n",
    "prompt=\"\"\"\n",
    "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
    "\n",
    "{doc}\n",
    "\"\"\"\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-16k')\n",
    "\n",
    "chain=LLMChain.from_string(\n",
    "    llm=llm,\n",
    "    template=prompt\n",
    ")\n",
    "\n",
    "chain.verbose=True\n",
    "chain.output_parser=NumberedListOutputParser()\n",
    "\n",
    "#pick and example\n",
    "chain.run(sl_data[20].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing a multimodal document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=Chroma(\n",
    "    collection_name=\"hypo-questions\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store=InMemoryStore()\n",
    "id_key='doc_id'\n",
    "\n",
    "retriever=MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids=[str(uuid.uuid4()) for _ in sl_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_docs=[]\n",
    "for i, doc in enumerate(sl_data):\n",
    "    result=chain.run(doc.page_content)\n",
    "    question_docs.extend([\n",
    "        Document(\n",
    "            page_contents=s,\n",
    "            metadata={id_key: doc_ids[i]}\n",
    "        ) for s in result\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, sl_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.similarity_search(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0)\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#insert question\n",
    "chain.run(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "You are an assistant tasked with summarizing tables and text\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "\n",
    "model=ChatOpenAI(temperature=0, model_name='gpt-4')\n",
    "summarize_chain=LLMChain.from_string(\n",
    "    llm=model,\n",
    "    template=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries=summarize_chain.batch(table_elements)\n",
    "text_summaries=summarize_chain.batch(text_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing images with Llava**\n",
    "\n",
    "LLAMA model merged with llava models \n",
    "\n",
    "- cd llama.cpp \n",
    "\n",
    "- mkdir build && cd build && cmake\n",
    "\n",
    "- cmake --build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/ggerganov/llama.cpp.git\n",
    "#pip install git-lfs or brew install\n",
    "#git lfs install \n",
    "#git clone https://huggingface.co/mys/ggml_llava-v1.5-7b\n",
    "#brew install cmake\n",
    "\n",
    "%%bash\n",
    "#Define directory containing images\n",
    "IMG_DIR=''\n",
    "TEXT_DIR=''\n",
    "\n",
    "#loop through each image in directory\n",
    "for img in \"${IMG_DIR}\"*.jpg: do\n",
    "    base_name=$(basename \"img\" .jpg)\n",
    "\n",
    "    output_file=\"${TEXT_DIR}${base_name}.txt\"\n",
    "\n",
    "    #model binaries\n",
    "    ~m 'output file path' \\\n",
    "    #file structure\n",
    "    --mmproj 'path'\\\n",
    "    --temp 0.1 'path'\\\n",
    "    -p \"Describe image in detail. Be specific about graphs, such as bar plots.\" \\\n",
    "    --image \"$img\" > \"output_file\" \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "text_path=\"\"\n",
    "images_path=\"\"\n",
    "\n",
    "text_list=sorted(glob.glob(text_path + \"*.txt\"))\n",
    "img_list=sorted(glob.glob(images_path+'*.jpg'))\n",
    "\n",
    "logging_header=\"clip_model_load: total allocated memory: 201.27 MB\\n\\n\"\n",
    "appendix=\"main: image encoded in\"\n",
    "\n",
    "img_summaries=[]\n",
    "for i, text_path in enumerate(text_list):\n",
    "    with open(text_path, 'r') as file:\n",
    "        summary=file.read()\n",
    "\n",
    "    summary=summary.split(logging_header, 1)[1].strip()\n",
    "    summary=summary.split(appendix, 1)[0].strip()\n",
    "\n",
    "    img_path=img_list[i]\n",
    "    img=Image.open(img_path)\n",
    "\n",
    "    img_summaries.append({\n",
    "        'summary':summary,\n",
    "        \"image\":img\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for img_dict in img_summaries:\n",
    "    display(img_dict['image'])\n",
    "    print(img_dict['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multimodal RAG Pipeline**\n",
    "\n",
    "option 1:\n",
    "\n",
    "Multimodal embedding generation (text or images). Ex: If GPT-4 allowed images. \n",
    "\n",
    "option 2:\n",
    "\n",
    "Convert different images into text descriptions then pass into multimodal LLM\n",
    "\n",
    "\n",
    "option 3 (available): \n",
    "\n",
    "Image converted to text to then answer questions using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index data into database\n",
    "\n",
    "def get_docs(text_list, ids):\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=s,\n",
    "            metadata={id_key: ids[i]}\n",
    "        ) for i, s in enumerate(text_list)\n",
    "    ]\n",
    "\n",
    "doc_ids=[str(uuid.uuid4()) for _ in text_summaries]\n",
    "text_docs=get_docs(\n",
    "    [t['elements'] for t in text_summaries],\n",
    "    doc_ids\n",
    ")\n",
    "\n",
    "summary_text_docs=get_docs(\n",
    "    [t['text'] for t in text_summaries],\n",
    "    doc_ids\n",
    ")\n",
    "\n",
    "table_ids=[str(uuid.uuid4()) for _ in table_summaries]\n",
    "table_docs=get_docs(\n",
    "    [t['element'] for t in table_summaries],\n",
    "    table_ids\n",
    ")\n",
    "\n",
    "summary_table_docs=get_docs(\n",
    "    [t['text'] for t in table_summaries], \n",
    "    table_ids\n",
    ")\n",
    "\n",
    "img_ids=[str(uuid.uuid4()) for _ in img_summaries]\n",
    "img_summary_docs=get_docs(\n",
    "    [i['summary'] for i in img_summaries], \n",
    "    img_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=Chroma(\n",
    "    collection_name='llava_pdf',\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store=InMemoryStore()\n",
    "\n",
    "retriever=MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_text_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, text_docs)))\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_table_docs)\n",
    "retriever.docstore.mset(list(zip(table_ids, table_docs))) \n",
    "\n",
    "retriever.vectorstore.add_documents(img_summary_docs)\n",
    "retriever.docstore.mset(list(zip(img_ids, img_summary_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert question\n",
    "retriever.vectorstore.similarity_search(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert question to compare \n",
    "retriever.get_relevant_documents(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalizing the Multimodal RAG\n",
    "llm=ChatOpenAI(temperature=0)\n",
    "\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#insert question \n",
    "chain.run('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting LLM with Graph Database (Benefits: Faster retrieval than raw text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "file_path=\"\"\n",
    "book_loader=PyPDFLoader(file_path=file_path)\n",
    "#list of documents\n",
    "book_data=book_loader.load_and_split()\n",
    "book_data[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a graph representation\n",
    "from langchain.indexes import GraphIndexCreator \n",
    "from langchain.llms import OpenAI \n",
    "\n",
    "llm=OpenAI(temperature=0)\n",
    "index_creator=GraphIndexCreator(llm=llm)\n",
    "graph=index_creator.from_text(book_data[20].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge triples (subject, predicate, and object)\n",
    "graph.get_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "graph.draw_graphviz(path=\"book.svg\")\n",
    "SVG('book.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert entire book into knowledge graph\n",
    "graphs=[\n",
    "    index_creator.from_text(doc.page_content)\n",
    "    for doc in book_data\n",
    "]\n",
    "#this creates many different network entity graphs for each element in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge different graphs\n",
    "import networkx as nx\n",
    "graph_nx=graphs[0]._graph\n",
    "for g in graphs[1:]:\n",
    "    graph_nx=nx.compose(graph_nx, g._graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs.networkx_graph import NetworkxEntityGraph\n",
    "\n",
    "graph=NetworkxEntityGraph(graph_nx)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.draw_graphviz(path=\"graph.pdf\", prog='fdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curate knowledge base using langchain\n",
    "\n",
    "from langchain.chains import GraphQAChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(temperature=0)\n",
    "\n",
    "chain=GraphQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "question='Question specific to the document source'\n",
    "\n",
    "chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "#query language for graph db by neo4j\n",
    "cypher_llm=ChatOpenAI(temperature=0, model_name='gpt-4')\n",
    "qa_llm=ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "\n",
    "chain=GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=cypher_llm,\n",
    "    qa_llm=qa_llm,\n",
    "    graph=graph_db,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting LLMs with Agent Tools\n",
    "\n",
    "Question->Thought (iterative)->Action (uses tool)->Action Input->Observation (Final Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\achen\\anaconda3\\envs\\py39\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=7cc63fba144dd5dd5aa01399c9d76ded2136ff42616abd2e4903677e3cc4b89b\n",
      "  Stored in directory: c:\\users\\achen\\appdata\\local\\pip\\cache\\wheels\\a8\\ca\\f6\\a3c8e5e97ce0a0beb22201fb53c8455979ea2ee676c95c9b8b\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "tools=load_tools(['wikipedia', 'llm-math'], llm=llm)\n",
    "\n",
    "agent=initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "question=\"\"\n",
    "handler=StdOutCallbackHandler()\n",
    "response=agent(\n",
    "    {\"input\":question},\n",
    "    callbacks=[handler]\n",
    ")\n",
    "\n",
    "#can filter with indices: ['input', 'output', 'intermediate_steps']\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Tools for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path=''\n",
    "\n",
    "loader=PyPDFLoader(file_path=file_path)\n",
    "data=loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n",
    "docsearch=Chroma.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    collection_name=\"statistical_learning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "llm=ChatOpenAI()\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "chain.run('question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "description=\"\"\"\n",
    "Answer questions about ML. \n",
    "Input should be a fully formed question\n",
    "\"\"\"\n",
    "\n",
    "retrieval_tool=Tool(\n",
    "    name=\"ML Knowledge\",\n",
    "    func=chain.run,\n",
    "    description=description,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=load_tools(['wikipedia', 'llm-math'], llm=llm)\n",
    "\n",
    "tools.append(retrieval_tool)\n",
    "\n",
    "agent=initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=''\n",
    "response=agent(\n",
    "    {\"input\":question},\n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM OPs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import inspect\n",
    "#dir(utils)\n",
    "#print(inspect.getsource(utils.authenticate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import train_test_split\n",
    "from datetime import datetime\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "#needs google api subscription\n",
    "def authenticate():\n",
    "    return \"DLAI_CREDENTIALS\", \"DLAI_PROJECT_ID\"\n",
    "    #Load .env\n",
    "    load_dotenv()\n",
    "    \n",
    "    #Decode key and store in .JSON\n",
    "    SERVICE_ACCOUNT_KEY_STRING_B64 = os.getenv('SERVICE_ACCOUNT_KEY')\n",
    "    SERVICE_ACCOUNT_KEY_BYTES_B64 = SERVICE_ACCOUNT_KEY_STRING_B64.encode(\"ascii\")\n",
    "    SERVICE_ACCOUNT_KEY_STRING_BYTES = base64.b64decode(SERVICE_ACCOUNT_KEY_BYTES_B64)\n",
    "    SERVICE_ACCOUNT_KEY_STRING = SERVICE_ACCOUNT_KEY_STRING_BYTES.decode(\"ascii\")\n",
    "\n",
    "    SERVICE_ACCOUNT_KEY = json.loads(SERVICE_ACCOUNT_KEY_STRING)\n",
    "\n",
    "\n",
    "    # Create credentials based on key from service account\n",
    "    # Make sure your account has the roles listed in the Google Cloud Setup section\n",
    "    credentials = Credentials.from_service_account_info(\n",
    "        SERVICE_ACCOUNT_KEY,\n",
    "        scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "    if credentials.expired:\n",
    "        credentials.refresh(Request())\n",
    "    \n",
    "    #Set project ID according to environment variable    \n",
    "    PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "        \n",
    "    return credentials, PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, PROJECT_ID=authenticate()\n",
    "\n",
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel \n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=REGION,\n",
    "              credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq_client=bigquery.Client(project=PROJECT_ID, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stackoverflow dataset\n",
    "\n",
    "QUERY_TABLES=\"\"\"\n",
    "SELECT table_name\n",
    "FROM bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES\n",
    "\"\"\"\n",
    "\n",
    "query_job=bq_client.query(QUERY_TABLES)\n",
    "\n",
    "for row in query_job:\n",
    "    for value in row.values():\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Retrieval\n",
    "\n",
    "INSPECT_QUERY=\"\"\"\n",
    "SELECT *\n",
    "FROM 'bigquery-public-data.stackoverflow.posts_questions' \n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "query_job=bq_client.query(INSPECT_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform results of query itno arrow table to then put into pandas df\n",
    "stack_overflow_df = query_job\\\n",
    "    .result()\\\n",
    "    .to_arrow()\\\n",
    "    .to_pandas()\n",
    "stack_overflow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with large datasets for LLMs\n",
    "\n",
    "QUERY_ALL=\"\"\" \n",
    "SELECT *\n",
    "FROM 'bigquery-public-data.stackoverflow.posts_questions' q\n",
    "\"\"\"\n",
    "\n",
    "query_job=bq_client.query(QUERY_ALL)\n",
    "\n",
    "try:\n",
    "    stack_overflow_df = query_job\\\n",
    "    .result()\\\n",
    "    .to_arrow()\\\n",
    "    .to_pandas()\n",
    "except Exception as e:\n",
    "     print('The DataFrame is too large to load into memory.', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When working with (large) data, query optimizing is needed in order to save time and resources.\n",
    "- Select questions as `input_text` (column 1), answers as `output_text` (column 2).\n",
    "- Take the questions from `posts_questions` and answers from `posts_answers`.\n",
    "- Join the questions and their corresponding accepted answers based on their same `unique ID`.\n",
    "- Making sure the question is about `Python`, and that it `has an answer`. And the date the question was posted is on or after `2020-01-01`\n",
    "- Limit as 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query optimization\n",
    "\n",
    "QUERY=\"\"\" \n",
    "SELECT CONCAT(q.title, q.body) as input_text, a.body AS output_text\n",
    "FROM 'bigquery-public-data.stackoverflow.posts_questions' q\n",
    "JOIN 'bigquery-public-data.stackoverflow.posts_answers' a\n",
    "ON q.accepted_answer_id=a.id\n",
    "WHERE q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND \n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT 10000\n",
    "\"\"\"\n",
    "\n",
    "query_job=bq_client.query(QUERY)\n",
    "\n",
    "stack_overflow_df = query_job.result()\\\n",
    "                        .to_arrow()\\\n",
    "                        .to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding instructions improves model performance and generalization to unseen tasks https://arxiv.org/pdf/2210.11416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python. \\\n",
    "Answer it like you are a developer answering Stackoverflow questions.\n",
    "\n",
    "Stackoverflow question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding instruction template to the original input text (questions)\n",
    "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\\n",
    "    + stack_overflow_df['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset preparation\n",
    "train, test=train_test_split(stack_overflow_df, test_size=0.2, random_state=42)\n",
    "\n",
    "#version controlling\n",
    "date=datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "cols=['input_text_instruct','output_text']\n",
    "tune_jsonl=train[cols].to_json(orient=\"records\", lines=True)\n",
    "training_data_filename=f\"train_data_stack_overflow_\\\n",
    "                        python_qa-{date},jsonl\"\n",
    "with open(training_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)\n",
    "\n",
    "tune_jsonl=test[cols].to_json(orient=\"records\", lines=True)\n",
    "testing_data_filename=f\"test_data_stack_overflow_\\\n",
    "                        python_qa-{date},jsonl\"\n",
    "with open(testing_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automation with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kubeflow pipelines (best practice to use .output format as inputs from each returned object, including after the return statement)\n",
    "from kfp import dsl, compiler \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=FutureWarning,\n",
    "                        module='kfp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must specify keyword arguments\n",
    "\n",
    "@dsl.component \n",
    "def preprocessing(datapath: str):\n",
    "    #preprocessing simplified example\n",
    "    input_df=pd.read_csv(datapath)\n",
    "    new_df=input_df.fillna(input_df.mean(), axis=1)\n",
    "    labels=new_df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test= train_test_split(new_df, labels, test_size=0.2, random_state=42)\n",
    "    X_train.to_csv('/path/to/X_train.csv', index=False)\n",
    "    X_test.to_csv('/path/to/X_test.csv', index=False)\n",
    "    y_train.to_csv('/path/to/y_train.csv', index=False)\n",
    "    y_test.to_csv('/path/to/y_test.csv', index=False)\n",
    "    return dsl.ContainerOp(\n",
    "        name='pp_task',\n",
    "        image='...',\n",
    "        command=['...'],\n",
    "        file_outputs={\n",
    "            'X_train': '/path/to/X_train',\n",
    "            'X_test': '/path/to/X_test',\n",
    "            'y_train': '/path/to/y_train',\n",
    "            'y_test': '/path/to/y_test',\n",
    "        }\n",
    "    )\n",
    "\n",
    "@dsl.component\n",
    "def training_step(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: np.array, y_test: np.array):\n",
    "    #training\n",
    "    trained_model=model.fit(X_train, y_train)\n",
    "    #base_model.save('/path/to/pretrained_model.h5')\n",
    "    val_pred=model.predict(X_test)\n",
    "    AUC_score=roc_auc_score(y_test,val_pred)\n",
    "\n",
    "    with open('/path/to/AUC_result.txt', 'w') as f:\n",
    "        f.write(str(AUC_score))\n",
    "\n",
    "    return dsl.ContainerOp(\n",
    "        name='trained_res',\n",
    "        image='...',\n",
    "        command=['...'],\n",
    "        arguments=['--X_train', X_train, '--X_test', X_test, '--Y_train', y_train, '--Y_test', y_test],\n",
    "        file_outputs={\n",
    "            'AUC_score':'/path/to/AUC_result.txt',\n",
    "            #'pretrained_model':'/path/to/pretrained_model.h5'\n",
    "        }\n",
    "    )\n",
    "\n",
    "@dsl.pipeline\n",
    "def llm_datapipeline(raw_data_path: str):\n",
    "    pp_task1=preprocessing(raw_data_path)\n",
    "    model_score_task2=training_step(X_train: pp_task1.outputs['X_train'], X_test: pp_task1.outputs['X_test'], y_train: pp_task1.outputs['y_train'], y_test: pp_task1.outputs['y_test'])\n",
    "    return model_score_task2.output['AUC'] #change to outputs if model included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the pipeline\n",
    "\n",
    "compiler.Compiler().compile(llm_datapipeline, 'llm_datapipeline.yaml')\n",
    "pipeline_arguments = {\n",
    "    \"raw_data\": \"/path/to/raw_data\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more in depth\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/\\\n",
    "large-language-model-pipelines/tune-large-model/v2.0.0'\n",
    "\n",
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"large_model_reference\": \"text-bison@001\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVAUATION_DATA_URI,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to view pipeline.yaml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat llm_datapipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import `PipelineJob` \n",
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=\"pipeline.yaml\",\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"deep_learning_ai_pipeline\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        ### {\"recipient\": \"World!\"} for this example\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=\"us-central1\",\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=\"./\",\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit()\n",
    "\n",
    "### check to see the status of the job\n",
    "job.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "pipeline_root \"./\"\n",
    "\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=template_path,\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"deep_learning_ai_pipeline-{date}\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=REGION,\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=pipeline_root,\n",
    "        ### enable_caching=True will save the outputs \n",
    "        ### of components for re-use, and will only re-run those\n",
    "        ### components for which the code or data has changed.\n",
    "        enable_caching=True,\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit()\n",
    "\n",
    "### check to see the status of the job\n",
    "job.state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment & Load Balancing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "#route traffic to different endpoints\n",
    "list_tuned_models=model.list_tuned_model_names()\n",
    "\n",
    "#randomly select from one of the endpoints to divide prediction load\n",
    "tuned_model_select=random.choice(list_tuned_models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a response. The prompt needs to be similar to the content model was trained on\n",
    "\n",
    "deployed_model=TextGenerationModel.get_tuned_model(tuned_model_select)\n",
    "PROMPT='How can I get the max value in a dictionary?'\n",
    "\n",
    "response=deployed_model.predict(PROMPT)\n",
    "\n",
    "#the response is stored in a dictionary format\n",
    "final_output=response._prediction_response[0][0][\"content\"]\n",
    "\n",
    "pprint(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt management templates \n",
    "\n",
    "instruct=\"\"\" \n",
    "Please answer the following StackOverflow question on Python. \\\n",
    "Answer it like \\\n",
    "you are a principal developer answering StackOverflow questions. \\\n",
    "Question:\n",
    "\"\"\"\n",
    "\n",
    "QUESTION = \"How can I store my TensorFlow checkpoint on\\\n",
    "Google Cloud Storage? Python example?\"\n",
    "\n",
    "Prompt=f\"\"\"\n",
    "{instruct}{QUESTION}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response=deployed_model.predict(PROMPT)\n",
    "output=final_response._prediction_response[0][0][\"content\"]\n",
    "\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain safety attributes of response\n",
    "\n",
    "blocked=final_response._prediction_response[0][0]\\\n",
    "['safetyAttributes']['blocked']\n",
    "\n",
    "print(blocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#citations\n",
    "\n",
    "citation=response._prediction_response[0][0]\\\n",
    "['citationMetadata']['citations']\n",
    "\n",
    "pprint(citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning & Evaluating (BLEU or ROUGE score)\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
